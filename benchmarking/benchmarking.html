<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Performance Benchmarking - MongoDB Driver Specifications</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../index.html">Introduction</a></li><li class="chapter-item expanded affix "><a href="../driver-mantras.html">Mantras</a></li><li class="chapter-item expanded affix "><a href="../wireversion-featurelist/wireversion-featurelist.html">Wire Version Feature List</a></li><li class="chapter-item expanded affix "><li class="part-title">Specifications</li><li class="chapter-item expanded "><div><strong aria-hidden="true">1.</strong> Serialization</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="../BSON.html"><strong aria-hidden="true">1.1.</strong> BSON</a></li><li class="chapter-item expanded "><a href="../bson-objectid/objectid.html"><strong aria-hidden="true">1.2.</strong> ObjectId</a></li><li class="chapter-item expanded "><a href="../bson-decimal128/decimal128.html"><strong aria-hidden="true">1.3.</strong> Decimal128</a></li><li class="chapter-item expanded "><a href="../bson-binary-uuid/uuid.html"><strong aria-hidden="true">1.4.</strong> UUID</a></li><li class="chapter-item expanded "><a href="../dbref/dbref.html"><strong aria-hidden="true">1.5.</strong> DBRef</a></li><li class="chapter-item expanded "><a href="../extended-json/extended-json.html"><strong aria-hidden="true">1.6.</strong> Extended JSON</a></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.</strong> Communication</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="../message/OP_MSG.html"><strong aria-hidden="true">2.1.</strong> OP_MSG</a></li><li class="chapter-item expanded "><a href="../run-command/run-command.html"><strong aria-hidden="true">2.2.</strong> Command Execution</a></li><li class="chapter-item expanded "><a href="../connection-string/connection-string-spec.html"><strong aria-hidden="true">2.3.</strong> Connection String</a></li><li class="chapter-item expanded "><a href="../uri-options/uri-options.html"><strong aria-hidden="true">2.4.</strong> URI Options</a></li><li class="chapter-item expanded "><a href="../ocsp-support/ocsp-support.html"><strong aria-hidden="true">2.5.</strong> OCSP</a></li><li class="chapter-item expanded "><a href="../mongodb-handshake/handshake.html"><strong aria-hidden="true">2.6.</strong> Initial Handshake</a></li><li class="chapter-item expanded "><a href="../compression/OP_COMPRESSED.html"><strong aria-hidden="true">2.7.</strong> Wire Compression</a></li><li class="chapter-item expanded "><a href="../socks5-support/socks5.html"><strong aria-hidden="true">2.8.</strong> SOCKS5</a></li><li class="chapter-item expanded "><a href="../initial-dns-seedlist-discovery/initial-dns-seedlist-discovery.html"><strong aria-hidden="true">2.9.</strong> Initial DNS Seedlist Discovery</a></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.</strong> Connectivity</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="../server-discovery-and-monitoring/server-discovery-and-monitoring.html"><strong aria-hidden="true">3.1.</strong> Server Discovery and Monitoring</a></li><li class="chapter-item expanded "><a href="../connection-monitoring-and-pooling/connection-monitoring-and-pooling.html"><strong aria-hidden="true">3.2.</strong> Connection Monitoring and Pooling</a></li><li class="chapter-item expanded "><a href="../load-balancers/load-balancers.html"><strong aria-hidden="true">3.3.</strong> Load Balancer Support</a></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.</strong> Availability</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="../server-discovery-and-monitoring/server-monitoring.html"><strong aria-hidden="true">4.1.</strong> Server Monitoring</a></li><li class="chapter-item expanded "><a href="../polling-srv-records-for-mongos-discovery/polling-srv-records-for-mongos-discovery.html"><strong aria-hidden="true">4.2.</strong> SRV Polling for mongos Discovery</a></li><li class="chapter-item expanded "><a href="../server-selection/server-selection.html"><strong aria-hidden="true">4.3.</strong> Server Selection</a></li><li class="chapter-item expanded "><a href="../max-staleness/max-staleness.html"><strong aria-hidden="true">4.4.</strong> Max Staleness</a></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded "><div><strong aria-hidden="true">5.</strong> Resilience</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">5.1.</strong> Retryability</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="../retryable-reads/retryable-reads.html"><strong aria-hidden="true">5.1.1.</strong> Reads</a></li><li class="chapter-item expanded "><a href="../retryable-writes/retryable-writes.html"><strong aria-hidden="true">5.1.2.</strong> Writes</a></li></ol></li><li class="chapter-item expanded "><a href="../client-side-operations-timeout/client-side-operations-timeout.html"><strong aria-hidden="true">5.2.</strong> CSOT</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">5.3.</strong> Consistency</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="../sessions/driver-sessions.html"><strong aria-hidden="true">5.3.1.</strong> Sessions</a></li><li class="chapter-item expanded "><a href="../causal-consistency/causal-consistency.html"><strong aria-hidden="true">5.3.2.</strong> Causal Consistency</a></li><li class="chapter-item expanded "><a href="../sessions/snapshot-sessions.html"><strong aria-hidden="true">5.3.3.</strong> Snapshot Reads</a></li><li class="chapter-item expanded "><a href="../transactions/transactions.html"><strong aria-hidden="true">5.3.4.</strong> Transactions</a></li><li class="chapter-item expanded "><a href="../transactions-convenient-api/transactions-convenient-api.html"><strong aria-hidden="true">5.3.5.</strong> Convenient Transactions API</a></li></ol></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded "><div><strong aria-hidden="true">6.</strong> Programmability</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">6.1.</strong> Resource Management</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="../enumerate-databases/enumerate-databases.html"><strong aria-hidden="true">6.1.1.</strong> Databases</a></li><li class="chapter-item expanded "><a href="../enumerate-collections/enumerate-collections.html"><strong aria-hidden="true">6.1.2.</strong> Collections</a></li><li class="chapter-item expanded "><a href="../index-management/index-management.html"><strong aria-hidden="true">6.1.3.</strong> Indexes</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">6.2.</strong> Data Management</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="../crud/crud.html"><strong aria-hidden="true">6.2.1.</strong> CRUD</a></li><li class="chapter-item expanded "><a href="../collation/collation.html"><strong aria-hidden="true">6.2.2.</strong> Collation</a></li><li class="chapter-item expanded "><a href="../server_write_commands/server_write_commands.html"><strong aria-hidden="true">6.2.3.</strong> Write Commands</a></li><li class="chapter-item expanded "><a href="../driver-bulk-update.html"><strong aria-hidden="true">6.2.4.</strong> Bulk API</a></li><li class="chapter-item expanded "><a href="../crud/bulk-write.html"><strong aria-hidden="true">6.2.5.</strong> Bulk Write</a></li><li class="chapter-item expanded "><a href="../read-write-concern/read-write-concern.html"><strong aria-hidden="true">6.2.6.</strong> R/W Concern</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">6.3.</strong> Cursors</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="../change-streams/change-streams.html"><strong aria-hidden="true">6.3.1.</strong> Change Streams</a></li><li class="chapter-item expanded "><a href="../find_getmore_killcursors_commands/find_getmore_killcursors_commands.html"><strong aria-hidden="true">6.3.2.</strong> find/getMore/killCursors</a></li></ol></li><li class="chapter-item expanded "><a href="../gridfs/gridfs-spec.html"><strong aria-hidden="true">6.4.</strong> GridFS</a></li><li class="chapter-item expanded "><a href="../versioned-api/versioned-api.html"><strong aria-hidden="true">6.5.</strong> Stable API</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">6.6.</strong> Security</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="../client-side-encryption/client-side-encryption.html"><strong aria-hidden="true">6.6.1.</strong> Client Side Encryption</a></li><li class="chapter-item expanded "><a href="../bson-binary-encrypted/binary-encrypted.html"><strong aria-hidden="true">6.6.2.</strong> BSON Binary Subtype 6</a></li></ol></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded "><div><strong aria-hidden="true">7.</strong> Observability</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="../command-logging-and-monitoring/command-logging-and-monitoring.html"><strong aria-hidden="true">7.1.</strong> Command Logging and Monitoring</a></li><li class="chapter-item expanded "><a href="../server-discovery-and-monitoring/server-discovery-and-monitoring-logging-and-monitoring.html"><strong aria-hidden="true">7.2.</strong> SDAM Logging and Monitoring</a></li><li class="chapter-item expanded "><a href="../logging/logging.html"><strong aria-hidden="true">7.3.</strong> Standardized Logging</a></li><li class="chapter-item expanded "><a href="../connection-monitoring-and-pooling/connection-monitoring-and-pooling.html"><strong aria-hidden="true">7.4.</strong> Connection Pool Logging</a></li></ol></li><li class="chapter-item expanded "><li class="spacer"></li><li class="chapter-item expanded "><div><strong aria-hidden="true">8.</strong> Testability</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="../unified-test-format/unified-test-format.html"><strong aria-hidden="true">8.1.</strong> Unified Test Format</a></li><li class="chapter-item expanded "><a href="../atlas-data-lake-testing/tests/index.html"><strong aria-hidden="true">8.2.</strong> Atlas Data Federation Testing</a></li><li class="chapter-item expanded "><a href="../benchmarking/benchmarking.html" class="active"><strong aria-hidden="true">8.3.</strong> Performance Benchmarking</a></li><li class="chapter-item expanded "><a href="../bson-corpus/bson-corpus.html"><strong aria-hidden="true">8.4.</strong> BSON Corpus</a></li><li class="chapter-item expanded "><a href="../connections-survive-step-down/tests/index.html"><strong aria-hidden="true">8.5.</strong> Replication Event Resilience</a></li><li class="chapter-item expanded "><a href="../faas-automated-testing/faas-automated-testing.html"><strong aria-hidden="true">8.6.</strong> FAAS Automated Testing</a></li><li class="chapter-item expanded "><a href="../serverless-testing/index.html"><strong aria-hidden="true">8.7.</strong> Atlas Serverless Testing</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">MongoDB Driver Specifications</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="performance-benchmarking"><a class="header" href="#performance-benchmarking">Performance Benchmarking</a></h1>
<ul>
<li>Status: Accepted</li>
<li>Minimum Server Version: N/A</li>
</ul>
<h2 id="abstract"><a class="header" href="#abstract">Abstract</a></h2>
<p>This document describes a standard benchmarking suite for MongoDB drivers.</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<h3 id="name-and-purpose"><a class="header" href="#name-and-purpose">Name and purpose</a></h3>
<p>Driver performance will be measured by the MongoDB Driver Performance Benchmark (AKA "DriverBench"). It will provide
both "horizontal" insights into how individual language driver performance evolves over time and "vertical" insights
into relative performance of different drivers.</p>
<p>We do expect substantial performance differences between language families (e.g. static vs. dynamic or compiled vs.
virtual-machine-based). However we still expect "vertical" comparison within language families to expose outlier
behavior that might be amenable to optimization.</p>
<h3 id="task-hierarchy"><a class="header" href="#task-hierarchy">Task Hierarchy</a></h3>
<p>The benchmark consists of a number of micro-benchmarks tasks arranged into groups of increasing complexity. This allows
us to better isolate areas within drivers that are faster or slower.</p>
<ul>
<li>BSON -- BSON encoding/decoding tasks, to explore BSON codec efficiency</li>
<li>Single-Doc -- single-document insertion and query tasks, to explore basic wire protocol efficiency</li>
<li>Multi-Doc -- multi-document insertion and query tasks, to explore batch-write and cursor chunking efficiency</li>
<li>Parallel -- multi-process/thread ETL tasks, to explore concurrent operation efficiency</li>
</ul>
<h3 id="measurement"><a class="header" href="#measurement">Measurement</a></h3>
<p>In addition to timing data, all micro-benchmark tasks will be measured in terms of "megabytes/second" (MB/s) of
documents processed, with higher scores being better. (In this document, "megabyte" refers to the SI decimal unit, i.e.
1,000,000 bytes.) This makes cross-benchmark comparisons easier.</p>
<p>To avoid various types of measurement skew, tasks will be measured over numerous iterations. Each iteration will have a
"scale" -- the number of similar operations performed -- that will vary by task. The final score for a task will be the
median score of the iterations. Other quantiles will be recorded for diagnostic analysis.</p>
<h3 id="data-sets"><a class="header" href="#data-sets">Data sets</a></h3>
<p>Data sets will vary by micro-benchmark. In some cases, they it will be a synthetically generated document inserted
repeatedly (with different <code>_id</code> fields) to construct an overall corpus of documents. In other cases, data sets will be
synthetic line-delimited JSON files or mock binary files.</p>
<h3 id="composite-scores"><a class="header" href="#composite-scores">Composite scores</a></h3>
<p>Micro-benchmark scores will be combined into a composite for each weight class ("BSONBench", "SingleBench", etc.) and
for read and write operations ("ReadBench" and "WriteBench"). The read and write scores will be combined into an
aggregate composite score ("DriverBench"). The compositing formula in the DriverBench uses simple averages with equal
weighting.</p>
<h3 id="versioning"><a class="header" href="#versioning">Versioning</a></h3>
<p>DriverBench will have vX.Y versioning. Minor updates and clarifications will increment "Y" and should have little impact
on score comparison. Major changes, such as changing score weights, MongoDB version tested against, or hardware used,
will increment "X" to indicate that older version scores are unlikely to be comparable.</p>
<h2 id="benchmark-execution-phases-and-measurement"><a class="header" href="#benchmark-execution-phases-and-measurement">Benchmark execution phases and measurement</a></h2>
<p>All micro-benchmark tasks will be conducted via a number of iterations. Each iteration will be timed and will generally
include a large number of individual driver operations.</p>
<p>We break up the measurement this way to better isolate the benchmark from external volatility. If we consider the
problem of benchmarking an operation over many iterations, such as 100,000 document insertions, we want to avoid two
extreme forms of measurement:</p>
<ul>
<li>measuring a single insertion 100,000 times -- in this case, the timing code is likely to be a greater proportion of
executed code, which could routinely evict the insertion code from CPU caches or mislead a JIT optimizer and throw off
results</li>
<li>measuring 100,000 insertions one time -- in this case, the longer the timer runs, the higher the likelihood that an
external event occurs that affects the time of the run</li>
</ul>
<p>Therefore, we choose a middle ground:</p>
<ul>
<li>measuring the same 1000 insertions over 100 iterations -- each timing run includes enough operations that insertion
code dominates timing code; unusual system events are likely to affect only a fraction of the 100 timing measurements</li>
</ul>
<p>With 100 timings of inserting the same 1000 documents, we build up a statistical distribution of the operation timing,
allowing a more robust estimate of performance than a single measurement. (In practice, the number of iterations could
exceed 100, but 100 is a reasonable minimum goal.)</p>
<p>Because a timing distribution is bounded by zero on one side, taking the mean would allow large positive outlier
measurements to skew the result substantially. Therefore, for the benchmark score, we use the median timing measurement,
which is robust in the face of outliers.</p>
<p>Each benchmark is structured into discrete setup/execute/teardown phases. Phases are as follows, with specific details
given in a subsequent section:</p>
<ul>
<li>setup -- (ONCE PER MICRO-BENCHMARK) something to do once before any benchmarking, e.g. construct a client object, load
test data, insert data into a collection, etc.</li>
<li>before task -- (ONCE PER ITERATION) something to do before every task iteration, e.g. drop a collection, or reload
test data (if the test run modifies it), etc.</li>
<li>do task -- (ONCE PER ITERATION) smallest amount of code necessary to execute the task; e.g. insert 1000 documents one
by one into the database, or retrieve 1000 document of test data from the database, etc.</li>
<li>after task -- (ONCE PER ITERATION) something to do after every task iteration (if necessary)</li>
<li>teardown -- (ONCE PER MICRO-BENCHMARK) something done once after all benchmarking is complete (if necessary); e.g.
drop the test database</li>
</ul>
<p>The wall-clock execution time of each "do task" phase will be recorded. We use wall clock time to model user experience
and as a lowest-common denominator across languages and threading models. Iteration timing should be done with a
high-resolution monotonic timer (or best language approximation).</p>
<p>Unless otherwise specified, the number of iterations to measure per micro-benchmark is variable:</p>
<ul>
<li>iterations should loop for at least 1 minute cumulative execution time</li>
<li>iterations should stop after 100 iterations or 5 minutes cumulative execution time, whichever is shorter</li>
</ul>
<p>This balances measurement stability with a timing cap to ensure all micro-benchmarks can complete in a reasonable time.
Languages with JIT compilers may do warm up iterations for which timings are discarded.</p>
<p>For each micro-benchmark, the 10th, 25th, 50th, 75th, 90th, 95th, 98th and 99th percentiles will be recorded using the
following algorithm:</p>
<ul>
<li>Given a 0-indexed array A of N iteration wall clock times</li>
<li>Sort the array into ascending order (i.e. shortest time first)</li>
<li>Let the index i for percentile p in the range [1,100] be defined as: <code>i = int(N * p / 100) - 1</code></li>
</ul>
<p><em>N.B. This is the <a href="https://en.wikipedia.org/wiki/Percentile#The_Nearest_Rank_method">Nearest Rank</a> algorithm, chosen for
its utter simplicity given that it needs to be implemented identically across multiple languages for every driver.</em></p>
<p>The 50th percentile (i.e. the median) will be used for score composition. Other percentiles will be stored for
visualizations and analysis (e.g. a "candlestick" chart showing benchmark volatility over time).</p>
<p>Each task will have defined for it an associated size in megabytes (MB). The score for micro-benchmark composition will
be the task size in MB divided by the median wall clock time.</p>
<h2 id="micro-benchmark-definitions"><a class="header" href="#micro-benchmark-definitions">Micro-benchmark definitions</a></h2>
<p>Datasets are available in the <code>data</code> directory adjacent to this spec.</p>
<p>Note: The term "LDJSON" means "line-delimited JSON", which should be understood to mean a collection of UTF-8 encoded
JSON documents (without embedded CR or LF characters), separated by a single LF character. (Some Internet definition of
line-delimited JSON use CRLF delimiters, but this benchmark uses only LF.)</p>
<h3 id="bson-micro-benchmarks"><a class="header" href="#bson-micro-benchmarks">BSON micro-benchmarks</a></h3>
<p>Datasets are in the 'extended_bson' tarball.</p>
<p>BSON tests focus on BSON encoding and decoding; they are client-side only and do not involve any transmission of data to
or from the benchmark server. When appropriate, data sets will be stored on disk as
<a href="https://www.mongodb.com/docs/manual/reference/mongodb-extended-json">extended strict JSON</a>. For drivers that don't
support extended JSON, a BSON analogue will be provided as well.</p>
<p>BSON micro-benchmarks include:</p>
<ul>
<li>Flat BSON Encoding and Flat BSON Decoding -- shallow documents with only common BSON field types</li>
<li>Deep BSON Encoding and Deep BSON Decoding -- deeply nested documents with only common BSON field types</li>
<li>Full BSON Encoding and Full BSON Decoding -- shallow documents with all possible BSON field types</li>
</ul>
<h4 id="flat-bson-encoding"><a class="header" href="#flat-bson-encoding">Flat BSON Encoding</a></h4>
<p>Summary: This benchmark tests driver performance encoding documents with top level key/value pairs involving the most
commonly-used BSON types.</p>
<p>Dataset: The dataset, designated FLAT_BSON (ftnt4 Disk file 'flat_bson.json'), will be synthetically generated and
consist of an extended JSON document with a single <code>_id</code> key with an object ID value plus 24 top level keys/value pairs
of the following types: string, Int32, Int64, Double, Boolean. (121 total key/value pairs) Keys will be random ASCII
strings of length 8. String data will be random ASCII strings of length 80.</p>
<p>Dataset size: For score purposes, the dataset size for a task is the size of the single-document source file (7531
bytes) times 10,000 operations, which equals 75,310,000 bytes or 75.31 MB.</p>
<div class="table-wrapper"><table><thead><tr><th>Phase</th><th>Description</th></tr></thead><tbody>
<tr><td>Setup</td><td>Load the FLAT_BSON dataset into memory as a language-appropriate document types. For languages like C without a document type, the raw JSON string for each document should be used instead.</td></tr>
<tr><td>Before task</td><td>n/a</td></tr>
<tr><td>Do task</td><td>Encode the FLAT_BSON document to a BSON byte-string. Repeat this 10,000 times.</td></tr>
<tr><td>After task</td><td>n/a</td></tr>
<tr><td>Teardown</td><td>n/a</td></tr>
</tbody></table>
</div>
<h4 id="flat-bson-decoding"><a class="header" href="#flat-bson-decoding">Flat BSON Decoding</a></h4>
<p>Summary: This benchmark tests driver performance decoding documents with top level key/value pairs involving the most
commonly-used BSON types.</p>
<p>Dataset: The dataset, designated FLAT_BSON, will be synthetically generated and consist of an extended JSON document
with a single <code>_id</code> key with an object ID value plus 24 top level keys/value pairs of each of the following types:
string, Int32, Int64, Double, Boolean. (121 total key/value pairs) Keys will be random ASCII strings of length 8. String
data will be random ASCII strings of length 80.</p>
<p>Dataset size: For score purposes, the dataset size for a task is the size of the single-document source file (7531
bytes) times 10,000 operations, which equals 75,310,000 bytes or 75.31 MB.</p>
<div class="table-wrapper"><table><thead><tr><th>Phase</th><th>Description</th></tr></thead><tbody>
<tr><td>Setup</td><td>Load the FLAT_BSON dataset into memory as a language-appropriate document types. For languages like C without a document type, the raw JSON string for each document should be used instead. Encode it to a BSON byte-string.</td></tr>
<tr><td>Before task</td><td>n/a</td></tr>
<tr><td>Do task</td><td>Decode the BSON byte-string to a language-appropriate document type. Repeat this 10,000 times. For languages like C without a document type, decode to extended JSON instead.</td></tr>
<tr><td>After task</td><td>n/a</td></tr>
<tr><td>Teardown</td><td>n/a</td></tr>
</tbody></table>
</div>
<h4 id="deep-bson-encoding"><a class="header" href="#deep-bson-encoding">Deep BSON Encoding</a></h4>
<p>Summary: This benchmark tests driver performance encoding documents with deeply nested key/value pairs involving
subdocuments, strings, integers, doubles and booleans.</p>
<p>Dataset: The dataset, designated DEEP_BSON (disk file 'deep_bson.json'), will be synthetically generated and consist of
an extended JSON document representing a balanced binary tree of depth 6, with "left" and "right" keys at each level
containing a sub-document until the final level, which will contain a random ASCII string of length 8 (126 total
key/value pairs).</p>
<p>Dataset size: For score purposes, the dataset size for a task is the size of the single-document source file (1964
bytes) times 10,000 operations, which equals 19,640,000 bytes or 19.64 MB.</p>
<div class="table-wrapper"><table><thead><tr><th>Phase</th><th>Description</th></tr></thead><tbody>
<tr><td>Setup</td><td>Load the DEEP_BSON dataset into memory as a language-appropriate document type. For languages like C without a document type, the raw JSON string for each document should be used instead.</td></tr>
<tr><td>Before task</td><td>n/a</td></tr>
<tr><td>Do task</td><td>Encode the DEEP_BSON document to a BSON byte-string. Repeat this 10,000 times.</td></tr>
<tr><td>After task</td><td>n/a</td></tr>
<tr><td>Teardown</td><td>n/a</td></tr>
</tbody></table>
</div>
<h4 id="deep-bson-decoding"><a class="header" href="#deep-bson-decoding">Deep BSON Decoding</a></h4>
<p>Summary: This benchmark tests driver performance decoding documents with deeply nested key/value pairs involving
subdocuments, strings, integers, doubles and booleans.</p>
<p>Dataset: The dataset, designated DEEP_BSON, will be synthetically generated and consist of an extended JSON document
representing a balanced binary tree of depth 6, with "left" and "right" keys at each level containing a sub-document
until the final level, which will contain a random ASCII string of length 8 (126 total key/value pairs).</p>
<p>Dataset size: For score purposes, the dataset size for a task is the size of the single-document source file (1964
bytes) times 10,000 operations, which equals 19,640,000 bytes or 19.64 MB.</p>
<div class="table-wrapper"><table><thead><tr><th>Phase</th><th>Description</th></tr></thead><tbody>
<tr><td>Setup</td><td>Load the DEEP_BSON dataset into memory as a language-appropriate document types. For languages like C without a document type, the raw JSON string for each document should be used instead. Encode it to a BSON byte-string.</td></tr>
<tr><td>Before task</td><td>n/a</td></tr>
<tr><td>Do task</td><td>Decode the BSON byte-string to a language-appropriate document type. Repeat this 10,000 times. For languages like C without a document type, decode to extended JSON instead.</td></tr>
<tr><td>After task</td><td>n/a</td></tr>
<tr><td>Teardown</td><td>n/a</td></tr>
</tbody></table>
</div>
<h4 id="full-bson-encoding"><a class="header" href="#full-bson-encoding">Full BSON Encoding</a></h4>
<p>Summary: This benchmark tests driver performance encoding documents with top level key/value pairs involving the full
range of BSON types.</p>
<p>Dataset: The dataset, designated FULL_BSON (disk file 'full_bson.json'), will be synthetically generated and consist of
an extended JSON document with a single <code>_id</code> key with an object ID value plus 6 each of the following types: string,
double, Int64, Int32, boolean, minkey, maxkey, array, binary data, UTC datetime, regular expression, Javascript code,
Javascript code with context, and timestamp. (91 total keys.) Keys (other than <code>_id</code>) will be random ASCII strings of
length 8. Strings values will be random ASCII strings with length 80.</p>
<p>Dataset size: For score purposes, the dataset size for a task is the size of the single-document source file (5734
bytes) times 10,000 operations, which equals 57,340,000 bytes or 57.34 MB.</p>
<div class="table-wrapper"><table><thead><tr><th>Phase</th><th>Description</th></tr></thead><tbody>
<tr><td>Setup</td><td>Load the FULL_BSON dataset into memory as a language-appropriate document type. For languages like C without a document type, the raw JSON string for each document should be used instead.</td></tr>
<tr><td>Before task</td><td>n/a</td></tr>
<tr><td>Do task</td><td>Encode the FULL_BSON document to a BSON byte-string. Repeat this 10,000 times.</td></tr>
<tr><td>After task</td><td>n/a</td></tr>
<tr><td>Teardown</td><td>n/a</td></tr>
</tbody></table>
</div>
<h4 id="full-bson-decoding"><a class="header" href="#full-bson-decoding">Full BSON Decoding</a></h4>
<p>Summary: This benchmark tests driver performance decoding documents with top level key/value pairs involving the full
range of BSON types.</p>
<p>Dataset: The dataset, designated FULL_BSON, will be synthetically generated and consist of an extended JSON document
with a single <code>_id</code> key with an object ID value plus 6 each of the following types: string, double, Int64, Int32,
boolean, minkey, maxkey, array, binary data, UTC datetime, regular expression, Javascript code, Javascript code with
context, and timestamp. (91 total keys.) Keys (other than <code>_id</code>) will be random ASCII strings of length 8. Strings
values will be random ASCII strings with length 80.</p>
<p>Dataset size: For score purposes, the dataset size for a task is the size of the single-document source file (5734
bytes) times 10,000 operations, which equals 57,340,000 bytes or 57.34 MB.</p>
<div class="table-wrapper"><table><thead><tr><th>Phase</th><th>Description</th></tr></thead><tbody>
<tr><td>Setup</td><td>Load the FULL_BSON dataset into memory as a language-appropriate document types. For languages like C without a document type, the raw JSON string for each document should be used instead. Encode it to a BSON byte-string.</td></tr>
<tr><td>Before task</td><td>n/a</td></tr>
<tr><td>Do task</td><td>Decode the BSON byte-string to a language-appropriate document type. Repeat this 10,000 times. For languages like C without a document type, decode to extended JSON instead.</td></tr>
<tr><td>After task</td><td>n/a</td></tr>
<tr><td>Teardown</td><td>n/a</td></tr>
</tbody></table>
</div>
<h3 id="single-doc-benchmarks"><a class="header" href="#single-doc-benchmarks">Single-Doc Benchmarks</a></h3>
<p>Datasets are in the 'single_and_multi_document' tarball.</p>
<p>Single-doc tests focus on single-document read and write operations. They are designed to give insights into the
efficiency of the driver's implementation of the basic wire protocol.</p>
<p>The data will be stored as strict JSON with no extended types.</p>
<p>Single-doc micro-benchmarks include:</p>
<ul>
<li>Run command</li>
<li>Find one by ID</li>
<li>Small doc insertOne</li>
<li>Large doc insertOne</li>
</ul>
<h4 id="run-command"><a class="header" href="#run-command">Run command</a></h4>
<p>Summary: This benchmark tests driver performance sending a command to the database and reading a response.</p>
<p>Dataset: n/a</p>
<p>Dataset size: While there is no external dataset, for score calculation purposes use 130,000 bytes (10,000 x the size of
a BSON {hello:true} command).</p>
<p><em>N.B. We use {hello:true} rather than {hello:1} to ensure a consistent command size.</em></p>
<div class="table-wrapper"><table><thead><tr><th>Phase</th><th>Description</th></tr></thead><tbody>
<tr><td>Setup</td><td>Construct a MongoClient object. Construct whatever language-appropriate objects (Database, etc.) would be required to send a command.</td></tr>
<tr><td>Before task</td><td>n/a</td></tr>
<tr><td>Do task</td><td>Run the command {hello:true} 10,000 times, reading (and discarding) the result each time.</td></tr>
<tr><td>After task</td><td>n/a</td></tr>
<tr><td>Teardown</td><td>n/a</td></tr>
</tbody></table>
</div>
<h4 id="find-one-by-id"><a class="header" href="#find-one-by-id">Find one by ID</a></h4>
<p>Summary: This benchmark tests driver performance sending an indexed query to the database and reading a single document
in response.</p>
<p>Dataset: The dataset, designated TWEET (disk file 'tweet.json'), consists of a sample tweet stored as strict JSON.</p>
<p>Dataset size: For score purposes, the dataset size for a task is the size of the single-document source file (1622
bytes) times 10,000 operations, which equals 16,220,000 bytes or 16.22 MB.</p>
<div class="table-wrapper"><table><thead><tr><th>Phase</th><th>Description</th></tr></thead><tbody>
<tr><td>Setup</td><td>Construct a MongoClient object. Drop the 'perftest' database. Load the TWEET document into memory as a language-appropriate document type (or JSON string for C). Construct a Collection object for the 'corpus' collection to use for querying. Insert the document 10,000 times to the 'perftest' database in the 'corpus' collection using sequential <code>_id</code> values. (1 to 10,000)</td></tr>
<tr><td>Before task</td><td>n/a</td></tr>
<tr><td>Do task</td><td>For each of the 10,000 sequential <code>_id</code> numbers, issue a find command for that <code>_id</code> on the 'corpus' collection and retrieve the single-document result.</td></tr>
<tr><td>After task</td><td>n/a</td></tr>
<tr><td>Teardown</td><td>Drop the 'perftest' database.</td></tr>
</tbody></table>
</div>
<h4 id="small-doc-insertone"><a class="header" href="#small-doc-insertone">Small doc insertOne</a></h4>
<p>Summary: This benchmark tests driver performance inserting a single, small document to the database.</p>
<p>Dataset: The dataset, designated SMALL_DOC (disk file 'small_doc.json'), consists of a JSON document with an encoded
length of approximately 250 bytes.</p>
<p>Dataset size: For score purposes, the dataset size for a task is the size of the single-document source file (275 bytes)
times 10,000 operations, which equals 2,750,000 bytes or 2.75 MB.</p>
<div class="table-wrapper"><table><thead><tr><th>Phase</th><th>Description</th></tr></thead><tbody>
<tr><td>Setup</td><td>Construct a MongoClient object. Drop the 'perftest' database. Load the SMALL_DOC dataset into memory as a language-appropriate document type (or JSON string for C).</td></tr>
<tr><td>Before task</td><td>Drop the 'corpus' collection. Create an empty 'corpus' collection with the 'create' command. Construct a Collection object for the 'corpus' collection to use for insertion.</td></tr>
<tr><td>Do task</td><td>Insert the document with the insertOne CRUD method. DO NOT manually add an <code>_id</code> field; leave it to the driver or database. Repeat this 10,000 times.</td></tr>
<tr><td>After task</td><td>n/a</td></tr>
<tr><td>Teardown</td><td>Drop the 'perftest' database.</td></tr>
</tbody></table>
</div>
<h4 id="large-doc-insertone"><a class="header" href="#large-doc-insertone">Large doc insertOne</a></h4>
<p>Summary: This benchmark tests driver performance inserting a single, large document to the database.</p>
<p>Dataset: The dataset, designated LARGE_DOC (disk file 'large_doc.json'), consists of a JSON document with an encoded
length of approximately 2,500,000 bytes.</p>
<p>Dataset size: For score purposes, the dataset size for a task is the size of the single-document source file (2,731,089
bytes) times 10 operations, which equals 27,310,890 bytes or 27.31 MB.</p>
<div class="table-wrapper"><table><thead><tr><th>Phase</th><th>Description</th></tr></thead><tbody>
<tr><td>Setup</td><td>Construct a MongoClient object. Drop the 'perftest' database. Load the LARGE_DOC dataset into memory as a language-appropriate document type (or JSON string for C).</td></tr>
<tr><td>Before task</td><td>Drop the 'corpus' collection. Create an empty 'corpus' collection with the 'create' command. Construct a Collection object for the 'corpus' collection to use for insertion.</td></tr>
<tr><td>Do task</td><td>Insert the document with the insertOne CRUD method. DO NOT manually add an <code>_id</code> field; leave it to the driver or database. Repeat this 10 times.</td></tr>
<tr><td>After task</td><td>n/a</td></tr>
<tr><td>Teardown</td><td>Drop the 'perftest' database.</td></tr>
</tbody></table>
</div>
<h3 id="multi-doc-benchmarks"><a class="header" href="#multi-doc-benchmarks">Multi-Doc Benchmarks</a></h3>
<p>Datasets are in the 'single_and_multi_document' tarball.</p>
<p>Multi-doc benchmarks focus on multiple-document read and write operations. They are designed to give insight into the
efficiency of the driver's implementation of bulk/batch operations such as bulk writes and cursor reads.</p>
<p>Multi-doc micro-benchmarks include:</p>
<ul>
<li>Find many and empty the cursor</li>
<li>Small doc bulk insert</li>
<li>Large doc bulk insert</li>
<li>GridFS upload</li>
<li>GridFS download</li>
</ul>
<h4 id="find-many-and-empty-the-cursor"><a class="header" href="#find-many-and-empty-the-cursor">Find many and empty the cursor</a></h4>
<p>Summary: This benchmark tests driver performance retrieving multiple documents from a query.</p>
<p>Dataset: The dataset, designated TWEET consists of a sample tweet stored as strict JSON.</p>
<p>Dataset size: For score purposes, the dataset size for a task is the size of the single-document source file (1622
bytes) times 10,000 operations, which equals 16,220,000 bytes or 16.22 MB.</p>
<div class="table-wrapper"><table><thead><tr><th>Phase</th><th>Description</th></tr></thead><tbody>
<tr><td>Setup</td><td>Construct a MongoClient object. Drop the 'perftest' database. Load the TWEET dataset into memory as a language-appropriate document type (or JSON string for C). Construct a Collection object for the 'corpus' collection to use for querying. Insert the document 10,000 times to the 'perftest' database in the 'corpus' collection. (Let the driver generate <code>_id</code>s).</td></tr>
<tr><td>Before task</td><td>n/a</td></tr>
<tr><td>Do task</td><td>Issue a find command on the 'corpus' collection with an empty filter expression. Retrieve (and discard) all documents from the cursor.</td></tr>
<tr><td>After task</td><td>n/a</td></tr>
<tr><td>Teardown</td><td>Drop the 'perftest' database.</td></tr>
</tbody></table>
</div>
<h4 id="small-doc-bulk-insert"><a class="header" href="#small-doc-bulk-insert">Small doc bulk insert</a></h4>
<p>Summary: This benchmark tests driver performance inserting multiple, small documents to the database.</p>
<p>Dataset: The dataset, designated SMALL_DOC consists of a JSON document with an encoded length of approximately 250
bytes.</p>
<p>Dataset size: For score purposes, the dataset size for a task is the size of the single-document source file (275 bytes)
times 10,000 operations, which equals 2,750,000 bytes or 2.75 MB.</p>
<div class="table-wrapper"><table><thead><tr><th>Phase</th><th>Description</th></tr></thead><tbody>
<tr><td>Setup</td><td>Construct a MongoClient object. Drop the 'perftest' database. Load the SMALL_DOC dataset into memory as a language-appropriate document type (or JSON string for C).</td></tr>
<tr><td>Before task</td><td>Drop the 'corpus' collection. Create an empty 'corpus' collection with the 'create' command. Construct a Collection object for the 'corpus' collection to use for insertion.</td></tr>
<tr><td>Do task</td><td>Do an ordered 'insert_many' with 10,000 copies of the document. DO NOT manually add an <code>_id</code> field; leave it to the driver or database.</td></tr>
<tr><td>After task</td><td>n/a</td></tr>
<tr><td>Teardown</td><td>Drop the 'perftest' database.</td></tr>
</tbody></table>
</div>
<h4 id="large-doc-bulk-insert"><a class="header" href="#large-doc-bulk-insert">Large doc bulk insert</a></h4>
<p>Summary: This benchmark tests driver performance inserting multiple, large documents to the database.</p>
<p>Dataset: The dataset, designated LARGE_DOC consists of a JSON document with an encoded length of approximately 2,500,000
bytes.</p>
<p>Dataset size: For score purposes, the dataset size for a task is the size of the single-document source file (2,731,089
bytes) times 10 operations, which equals 27,310,890 bytes or 27.31 MB.</p>
<div class="table-wrapper"><table><thead><tr><th>Phase</th><th>Description</th></tr></thead><tbody>
<tr><td>Setup</td><td>Construct a MongoClient object. Drop the 'perftest' database. Load the LARGE_DOC dataset into memory as a language-appropriate document type (or JSON string for C).</td></tr>
<tr><td>Before task</td><td>Drop the 'corpus' collection. Create an empty 'corpus' collection with the 'create' command. Construct a Collection object for the 'corpus' collection to use for insertion.</td></tr>
<tr><td>Do task</td><td>Do an ordered 'insert_many' with 10 copies of the document. DO NOT manually add an <code>_id</code> field; leave it to the driver or database.</td></tr>
<tr><td>After task</td><td>n/a</td></tr>
<tr><td>Teardown</td><td>Drop the 'perftest' database.</td></tr>
</tbody></table>
</div>
<h4 id="gridfs-upload"><a class="header" href="#gridfs-upload">GridFS upload</a></h4>
<p>Summary: This benchmark tests driver performance uploading a GridFS file from memory.</p>
<p>Dataset: The dataset, designated GRIDFS_LARGE (disk file 'gridfs_large.bin'), consists of a single file containing about
50 MB of random data. We use a large file to ensure multiple database round-trips even if chunks are are sent in
batches.</p>
<p>Dataset size: For score purposes, the dataset size for a task is the size of the source file (52,428,800 bytes) times 1
operation or 52.43 MB.</p>
<div class="table-wrapper"><table><thead><tr><th>Phase</th><th>Description</th></tr></thead><tbody>
<tr><td>Setup</td><td>Construct a MongoClient object. rop the 'perftest' database. Load the GRIDFS_LARGE file as a string or other language-appropriate type for binary octet data.</td></tr>
<tr><td>Before task</td><td>Drop the default GridFS bucket. Insert a 1-byte file into the bucket. (This ensures the bucket collections and indices have been created.) Construct a GridFSBucket object to use for uploads.</td></tr>
<tr><td>Do task</td><td>Upload the GRIDFS_LARGE data as a GridFS file. Use whatever upload API is most natural for each language (e.g. open_upload_stream(), write the data to the stream and close the stream).</td></tr>
<tr><td>After task</td><td>n/a</td></tr>
<tr><td>Teardown</td><td>Drop the 'perftest' database.</td></tr>
</tbody></table>
</div>
<h4 id="gridfs-download"><a class="header" href="#gridfs-download">GridFS download</a></h4>
<p>Summary: This benchmark tests driver performance downloading a GridFS file to memory.</p>
<p>Dataset: The dataset, designated GRIDFS_LARGE, consists of a single file containing about 50 MB of random data. We use a
large file to ensure multiple database round-trips even if chunks are are sent in batches.</p>
<p>Dataset size: For score purposes, the dataset size for a task is the size of the source file (52,428,800 bytes) times 1
operation or 52.43 MB.</p>
<div class="table-wrapper"><table><thead><tr><th>Phase</th><th>Description</th></tr></thead><tbody>
<tr><td>Setup</td><td>Construct a MongoClient object. Drop the 'perftest' database. Upload the GRIDFS_LARGE file to the default gridFS bucket with the name "gridfstest". Record the <code>_id</code> of the uploaded file.</td></tr>
<tr><td>Before task</td><td>Construct a GridFSBucket object to use for downloads.</td></tr>
<tr><td>Do task</td><td>Download the "gridfstest" file by its <code>_id</code>. Use whatever download API is most natural for each language (e.g. open_download_stream(), read from the stream into a variable). Discard the downloaded data.</td></tr>
<tr><td>After task</td><td>n/a</td></tr>
<tr><td>Teardown</td><td>Drop the 'perftest' database.</td></tr>
</tbody></table>
</div>
<h3 id="parallel"><a class="header" href="#parallel">Parallel</a></h3>
<p>Datasets are in the 'parallel' tarball.</p>
<p>Parallel tests simulate ETL operations from disk to database or vice-versa. They are designed to be implemented using a
language's preferred approach to concurrency and thus stress how drivers handle concurrency. These intentionally involve
overhead above and beyond the driver itself to simulate -- however loosely -- the sort of "real-world" pressures that a
drivers would be under during concurrent operation.</p>
<p>They are intended for directional indication of which languages perform best for this sort of pseudo-real-world
activity, but are not intended to represent real-world performance claims.</p>
<p>Drivers teams are expected to treat these as a competitive "shoot-out" to surface optimal ETL patterns for each language
(e.g. multi-thread, multi-process, asynchronous I/O, etc.).</p>
<p>Parallel micro-benchmarks include:</p>
<ul>
<li>LDJSON multi-file import</li>
<li>LDJSON multi-file export</li>
<li>GridFS multi-file upload</li>
<li>GridFS multi-file download</li>
</ul>
<h4 id="ldjson-multi-file-import"><a class="header" href="#ldjson-multi-file-import">LDJSON multi-file import</a></h4>
<p>Summary: This benchmark tests driver performance importing documents from a set of LDJSON files.</p>
<p>Dataset: The dataset, designated LDJSON_MULTI (disk directory 'ldjson_multi'), consists of 100 LDJSON files, each
containing 5,000 JSON documents. Each document should be about 1000 bytes.</p>
<p>Dataset size: For score purposes, the dataset size for a task is the total size of all source files: 565,000,000 bytes
or 565 MB.</p>
<div class="table-wrapper"><table><thead><tr><th>Phase</th><th>Description</th></tr></thead><tbody>
<tr><td>Setup</td><td>Construct a MongoClient object. Drop the 'perftest' database.</td></tr>
<tr><td>Before task</td><td>Drop the 'corpus' collection. Create an empty 'corpus' collection with the 'create' command.</td></tr>
<tr><td>Do task</td><td>Do an unordered insert of all 500,000 documents in the dataset into the 'corpus' collection as fast as possible.  Data must be loaded from disk during this phase.  Concurrency is encouraged.</td></tr>
<tr><td>After task</td><td>n/a</td></tr>
<tr><td>Teardown</td><td>Drop the 'perftest' database.</td></tr>
</tbody></table>
</div>
<h4 id="ldjson-multi-file-export"><a class="header" href="#ldjson-multi-file-export">LDJSON multi-file export</a></h4>
<p>Summary: This benchmark tests driver performance exporting documents to a set of LDJSON files.</p>
<p>Dataset: The dataset, designated LDJSON_MULTI, consists of 100 LDJSON files, each containing 5,000 JSON documents. Each
document should be about 1000 bytes.</p>
<p>Dataset size: For score purposes, the dataset size for a task is the total size of all source files: 565,000,000 bytes
or 565 MB.</p>
<div class="table-wrapper"><table><thead><tr><th>Phase</th><th>Description</th></tr></thead><tbody>
<tr><td>Setup</td><td>Construct a MongoClient object. Drop the 'perftest' database. Drop the 'corpus' collection. Do an unordered insert of all 500,000 documents in the dataset into the 'corpus' collection.</td></tr>
<tr><td>Before task</td><td>Construct whatever objects, threads, etc. are required for exporting the dataset.</td></tr>
<tr><td>Do task</td><td>Dump all 500,000 documents in the dataset into 100 LDJSON files of 5,000 documents each as fast as possible. Data must be completely written/flushed to disk during this phase. Concurrency is encouraged. The order and distribution of documents across files does not need to match the original LDJSON_MULTI files.</td></tr>
<tr><td>After task</td><td>n/a</td></tr>
<tr><td>Teardown</td><td>Drop the 'perftest' database.</td></tr>
</tbody></table>
</div>
<h4 id="gridfs-multi-file-upload"><a class="header" href="#gridfs-multi-file-upload">GridFS multi-file upload</a></h4>
<p>Summary: This benchmark tests driver performance uploading files from disk to GridFS.</p>
<p>Dataset: The dataset, designated GRIDFS_MULTI (disk directory 'gridfs_multi'), consists of 50 files, each of 5MB. This
file size corresponds roughly to the output of a (slightly dated) digital camera. Thus the task approximates uploading
50 "photos".</p>
<p>Dataset size: For score purposes, the dataset size for a task is the total size of all source files: 262,144,000 bytes
or 262.144 MB.</p>
<div class="table-wrapper"><table><thead><tr><th>Phase</th><th>Description</th></tr></thead><tbody>
<tr><td>Setup</td><td>Construct a MongoClient object. Drop the 'perftest' database.</td></tr>
<tr><td>Before task</td><td>Drop the default GridFS bucket in the 'perftest' database.  Construct a GridFSBucket object for the default bucket in 'perftest' to use for uploads.    Insert a 1-byte file into the bucket (to initialize indexes).</td></tr>
<tr><td>Do task</td><td>Upload all 50 files in the  GRIDFS_MULTI dataset (reading each from disk). Concurrency is encouraged.</td></tr>
<tr><td>After task</td><td>n/a</td></tr>
<tr><td>Teardown</td><td>Drop the 'perftest' database.</td></tr>
</tbody></table>
</div>
<h4 id="gridfs-multi-file-download"><a class="header" href="#gridfs-multi-file-download">GridFS multi-file download</a></h4>
<p>Summary: This benchmark tests driver performance downloading files from GridFS to disk.</p>
<p>Dataset: The dataset, designated GRIDFS_MULTI, consists of 50 files, each of 5MB. This file size corresponds roughly to
the output of a (slightly dated) digital camera. Thus the task approximates downloading 50 "photos".</p>
<p>Dataset size: For score purposes, the dataset size for a task is the total size of all source files: 262,144,000 bytes
or 262.144 MB.</p>
<div class="table-wrapper"><table><thead><tr><th>Phase</th><th>Description</th></tr></thead><tbody>
<tr><td>Setup</td><td>Construct a MongoClient object. Drop the 'perftest' database. Construct a temporary directory for holding downloads. Drop the default GridFS bucket in the 'perftest' database. Upload the 50 file dataset to the default GridFS bucket in 'perftest'.</td></tr>
<tr><td>Before task</td><td>Delete all files in the temporary folder for downloads. Construct a GridFSBucket object to use for downloads from the default bucket in 'perftest'.</td></tr>
<tr><td>Do task</td><td>Download all 50 files in the GRIDFS_MULTI dataset, saving each to a file in the temporary folder for downloads. Data must be completely written/flushed to disk during this phase. Concurrency is encouraged.</td></tr>
<tr><td>After task</td><td>n/a</td></tr>
<tr><td>Teardown</td><td>Drop the 'perftest' database.</td></tr>
</tbody></table>
</div>
<h2 id="composite-score-calculation"><a class="header" href="#composite-score-calculation">Composite score calculation</a></h2>
<p>Every micro-benchmark has a score equal to the 50th percentile (median) of sampled timings, expressed as Megabytes
(1,000,000 bytes) per second where the micro-benchmark "database size" given in each section above is divided by the
50th percentile of measured wall-clock times.</p>
<p>From these micro-benchmarks, the following composite scores must be calculated:</p>
<div class="table-wrapper"><table><thead><tr><th>Composite Name</th><th>Compositing formula</th></tr></thead><tbody>
<tr><td>BSONBench</td><td>Average of all BSON micro-benchmarks</td></tr>
<tr><td>SingleBench</td><td>Average of all Single-doc micro-benchmarks, except "Run Command"</td></tr>
<tr><td>MultiBench</td><td>Average of all Multi-doc micro-benchmarks</td></tr>
<tr><td>ParallelBench</td><td>Average of all Parallel micro-benchmarks</td></tr>
<tr><td>ReadBench</td><td>Average of "Find one", "Find many and empty cursor", "GridFS download", "LDJSON multi-file export", and "GridFS multi-file download" microbenchmarks</td></tr>
<tr><td>WriteBench</td><td>Average of "Small doc insertOne", "Large doc insertOne", "Small doc bulk insert", "Large doc bulk insert", "GridFS upload", "LDJSON multi-file import", and "GridFS multi-file upload" micro-benchmarks</td></tr>
<tr><td>DriverBench</td><td>Average of ReadBench and WriteBench</td></tr>
</tbody></table>
</div>
<p>At least for this first DriverBench version, scores are combined with simple averages. In addition, the BSONBench scores
do not factor into the overall DriverBench scores, as encoding and decoding are inherent in all other tasks.</p>
<h2 id="benchmark-platform-configuration-and-environments"><a class="header" href="#benchmark-platform-configuration-and-environments">Benchmark platform, configuration and environments</a></h2>
<h3 id="benchmark-client"><a class="header" href="#benchmark-client">Benchmark Client</a></h3>
<p>TBD: spec Amazon instance size; describe in general terms how language clients will be run independently; same AWS zone
as server</p>
<p>All operations must be run with write concern "w:1".</p>
<h3 id="benchmark-server"><a class="header" href="#benchmark-server">Benchmark Server</a></h3>
<p>TBD: spec Amazon instance size; describe configuration (e.g. no auth, journal, pre-alloc sizes?, WT with compression to
minimize disk I/O impact?); same AWS zone as client</p>
<h3 id="score-server"><a class="header" href="#score-server">Score Server</a></h3>
<p>TBD: spec system to hold scores over time</p>
<h3 id="datasets"><a class="header" href="#datasets">Datasets</a></h3>
<p>TBD: generated datasets should be park in S3 or somewhere for retrieval by URL</p>
<h2 id="changelog"><a class="header" href="#changelog">Changelog</a></h2>
<ul>
<li>
<p>2024-01-22: Migrated from reStructuredText to Markdown.</p>
</li>
<li>
<p>2022-10-05: Remove spec front matter and reformat changelog.</p>
</li>
<li>
<p>2021-04-06: Update run command test to use 'hello' command</p>
</li>
<li>
<p>2016-08-13:</p>
<ul>
<li>Update corpus files to allow much greater compression of data</li>
<li>Updated LDJSON corpus size to reflect revisions to the test data</li>
<li>Published data files on GitHub and updated instructions on how to find datasets</li>
<li>RunCommand and query benchmark can create collection objects during setup rather than before task. (No change on
actual benchmark.)</li>
</ul>
</li>
<li>
<p>2016-01-06:</p>
<ul>
<li>Clarify that 'bulk insert' means 'insert_many'</li>
<li>Clarify that "create a collection" means using the 'create' command</li>
<li>Add omitted "upload files" step to setup for GridFS multi-file download; also clarify that steps should be using the
default bucket in the 'perftest' database</li>
</ul>
</li>
<li>
<p>2015-12-23:</p>
<ul>
<li>Rename benchmark names away from MMA/weight class names</li>
<li>Split BSON encoding and decoding micro-benchmarks</li>
<li>Rename BSON micro-benchmarks to better match dataset names</li>
<li>Move "Run Command" micro-benchmark out of composite</li>
<li>Reduced amount of data held in memory and sent to/from the server to decrease memory pressure and increase number of
iterations in a reasonable time (e.g. file sizes and number of documents in certain datasets changed)</li>
<li>Create empty collections/indexes during the 'before' phase when appropriate</li>
<li>Updated data set sizes to account for changes in the source file structure/size</li>
</ul>
</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../atlas-data-lake-testing/tests/index.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../bson-corpus/bson-corpus.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../atlas-data-lake-testing/tests/index.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../bson-corpus/bson-corpus.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
